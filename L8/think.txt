Thinking1	在CTR点击率预估中，使用GBDT+LR的原理是什么？	
    GBDT在CTR预估中的作用是自动特征组合， 根据样本落在树模型的叶子节点上,GBDT树模型采用节点的分裂进行特征交叉，树模型的深度决定了特征交叉的维度。
    逻辑回归（logistics regression）作为广义线性模型的一种，它的假设是因变量y服从伯努利分布。
    那么在点击率预估这个问题上，“点击”这个事件是否发生就是模型的因变量y。而用户是否点击广告这个问题是一个经典的掷偏心硬币（二分类）问题，
    因此CTR模型的因变量显然应该服从伯努利分布。所以采用LR作为CTR模型是符合“点击”这一事件的物理意义的。
    LR模型处理高位离散特征快捷有效， 且具有一定的泛化能力。 
    GBDT+LR 一定程度上解决了人工特征工程的不足， 比单独LR 或者GBDT效果更优。 
Thinking2	Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力？ 	
    wide&deep 模型作为一种深度学习的框架， 将离散特征和连续特征进行区分对待 ,
    一般而言， 离散特征需要通过OneHot编码进行转换 => 高维稀疏特征 , Concatenate， 需要人工进行特征交叉来提取交叉特征， 之后送入wide侧,wide侧采用Linear Regression，解决模型的记忆能力
    而deep侧的输入一般是 连续特征或者 离散特征embedding之后的特征向量。 使用神经网络进行训练， 提取更高维度的交叉信息, 实现模型的泛化能力。 
    通过Wide & deep 结果加权进行预测。
Thinking3	在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？		
    两种FM和DNN的结合方式：
        • DeepFM, 并行结构，FM和DNN分开计算 , 只是在输出层进行一次融合得到结果 。 
        • NFM, AFM 串行架构，将FM的一维特征和二维特征作为DNN的输入 ， 通过dnn 输入结果 。 
    FM和DNN的组合出的算法:
        DeepFM  ,AFM ,FNM 
Thinking4	Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？	 
    KNNBasic是基本的CF算法，user-based或者item-based 
    KNNWithMeans 基于的一个假设也是用户和item的评分有高低，去除一个平均值后再计算 了 ， 考虑每个用户的平均评分
    KNNBaseline 是在 KNNWithMeans 基础上，用baseline的值来替换均值 ，协同过滤算法的变种，考虑每个用户评分的基线
    Baseline算法：基于统计的基准预测线打分 ,考虑的是全局评分的基线 。 
Thinking5	GBDT和随机森林都是基于树的算法，它们有什么区别？			
	一，随机森林
		随机森林是一个用随机方式建立的，包含多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。
		假设样本总数为n，每个样本的特征数为a，则随机森林的生成过程如下：
			1.从原始样本中采用有放回抽样的方法选取n个样本；
			2.对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点；
			3.重复m次，获得m个决策树；
			4. 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。
		随机森林的随机性主要体现在两个方面：

			1. 数据集的随机选取：从原始的数据集中采取有放回的抽样（bagging），构造子数据集，子数据集的数据量是和原始数据集相同的。
					不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。
			2. 待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，
					而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。
			以上两个随机性能够使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。

		随机森林的优点：
			实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
			相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
			能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
			对于不平衡的数据集，可以平衡误差；
			相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
			训练完成后可以给出哪些特征比较重要。
		随机森林的缺点：
			在噪声过大的分类和回归问题还是容易过拟合；
			相比于单一决策树，它的随机性让我们难以对模型进行解释。
	二，GBDT （Gradient Boost Decision Tree 梯度提升决策树）
		GBDT是以决策树为基学习器的迭代算法，注意GBDT里的决策树都是回归树而不是分类树  。 
		一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。
		GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。
		比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，
		即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；
		如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。
		
		GBDT优点是适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。
		缺点是由于弱分类器的串行依赖，导致难以并行训练数据。

	三，随机森林和GBDT的区别：
		随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，
		Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，
		而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），
		因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，
		而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
		组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
		组成随机森林的树可以并行生成；而GBDT只能是串行生成。

		对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
		随机森林对异常值不敏感；GBDT对异常值非常敏感。
		随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
		随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。

Thinking6	基于邻域的协同过滤都有哪些算法，请简述原理		
    基于邻域的协同过滤都有哪些算法有userCF , itemCF 。 
    协同过滤算法都是基于user, item之间的交互矩阵进行计算， 可以是显示的交互矩阵 ， 也可以是隐形的交互矩阵， 。 
    UserCf 计算与待推荐用户最为相似的k个用户行为过的物品推荐给当前用户系统维持一张用户相似度表 。 
    itemCF 推荐和当前用户历史上行为过的物品相似的物品给当前用户(可解释性强)
    用户数远大于物品数时，使用采用ItemCF；用户数少于物品数时，使用UserCF更准确 。 
    如果物品列表经常变换，那么采用UserCF更准确 ；
    如果物品列表相对于用户更稳定，那么采用ItemCF 。 
