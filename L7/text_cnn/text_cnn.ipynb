{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入所有需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np \n",
    "import random \n",
    "import string \n",
    "import  tensorflow.keras as keras \n",
    "import re \n",
    "from collections import Counter\n",
    "import jieba\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载训练文件  采样处理的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "train_file_path =\"sample__train_0.2.txt\"\n",
    "valid_file_path =\"sample__valid_0.2.txt\"\n",
    "test_file_path =\"sample__test_0.2.txt\" \n",
    "base_dir = os.path.curdir\n",
    "file_list = [train_file_path ,valid_file_path  ,test_file_path] \n",
    "for i , filename in enumerate( file_list ) :\n",
    "    file_list[i] = os.path.join( \n",
    "        os.path.abspath(base_dir) , filename) \n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  read_file 解析文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    read_file \n",
    "    return label , content  use jieba lcut function \n",
    "    \"\"\"\n",
    "    re_han = re.compile(u\"([\\u4E00-\\u9FD5a-zA-Z]+)\")  # the method of cutting text by punctuation\n",
    "    contents,labels=[],[]\n",
    "    with codecs.open(filename,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line=line.rstrip()\n",
    "                assert len(line.split('\\t'))==2\n",
    "                label,content=line.split('\\t')\n",
    "                labels.append(label)\n",
    "                blocks = re_han.split(content)\n",
    "                word = []\n",
    "                for blk in blocks:\n",
    "                    if re_han.match(blk):\n",
    "                        for w in jieba.cut(blk):\n",
    "                            if len(w)>=2:\n",
    "                                word.append(w)\n",
    "                contents.append(word)\n",
    "            except:\n",
    "                pass\n",
    "    return labels,contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filenames,vocab_dir,vocab_size=8000):\n",
    "    all_data = []\n",
    "    for filename in filenames:\n",
    "        _,data_train=read_file(filename)\n",
    "        for content in data_train:\n",
    "            all_data.extend(content)\n",
    "    counter=Counter(all_data)\n",
    "    count_pairs=counter.most_common(vocab_size-1)\n",
    "    words,_=list(zip(*count_pairs))\n",
    "    words=['<PAD>']+list(words)\n",
    "    with codecs.open(vocab_dir,'w',encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(words)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dir =\"vocab.txt\"\n",
    "vocab_size = 8000\n",
    "build_vocab(filenames= file_list,\n",
    "            vocab_dir =vocab_dir,\n",
    "            vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "    words=codecs.open(vocab_dir,'r',encoding='utf-8').read()\\\n",
    "            .strip().split('\\n')\n",
    "    word_to_id=dict(zip(words,range(len(words))))\n",
    "    return words,word_to_id\n",
    "\n",
    "def read_category():\n",
    "    categories = ['体育', '财经', '房产', '家居', \n",
    "                  '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    cat_to_id=dict(zip(categories,range(len(categories))))\n",
    "    return categories,cat_to_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dir =\"vocab.txt\"\n",
    "words,word_to_id = read_vocab(vocab_dir)\n",
    "categories,cat_to_id = read_category()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 训练词向量  ，利用word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.生成semtences  , 必须是可迭代的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_han= re.compile(u\"([\\u4E00-\\u9FD5a-zA-Z]+)\") # the method of cutting text by punctuation\n",
    "\n",
    "class Make_Sentences(object):\n",
    "    def __init__(self,filenames):\n",
    "        self.filenames= filenames\n",
    "\n",
    "    def __iter__(self):\n",
    "        for filename in self.filenames:\n",
    "            with codecs.open(filename, 'r', encoding='utf-8') as f:\n",
    "                for _,line in enumerate(f):\n",
    "                    try:\n",
    "                        line=line.strip()\n",
    "                        line=line.split('\\t')\n",
    "                        assert len(line)==2\n",
    "                        blocks=re_han.split(line[1])\n",
    "                        word=[]\n",
    "                        for blk in blocks:\n",
    "                            if re_han.match(blk):\n",
    "                                word.extend(jieba.lcut(blk))\n",
    "                        yield word\n",
    "                    except:\n",
    "                        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "def train_word2vec(filenames ,vector_word_filename):\n",
    "    import time\n",
    "    t1 = time.time()\n",
    "    sentences = Make_Sentences(filenames)\n",
    "    model = word2vec.Word2Vec(sentences, \n",
    "                                size=100,\n",
    "                                  window=5,\n",
    "                               min_count=1,\n",
    "                              workers=4)\n",
    "    model.wv.save_word2vec_format(vector_word_filename, binary=False)\n",
    "    print('-------------------------------------------')\n",
    "    print(\"Training word2vec model cost %.3f seconds...\\n\" % \\\n",
    "          (time.time() - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  或者加入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_word2vec_vectors(vocab, word2vec_dir,trimmed_filename):\n",
    "    file_r = codecs.open(word2vec_dir, 'r', encoding='utf-8')\n",
    "    line = file_r.readline()\n",
    "    voc_size, vec_dim = map(int, line.split(' '))\n",
    "    embeddings = np.zeros([len(vocab), vec_dim])\n",
    "    line = file_r.readline()\n",
    "    while line:\n",
    "        try:\n",
    "            items = line.split(' ')\n",
    "            word = items[0]\n",
    "            vec = np.asarray(items[1:], dtype='float32')\n",
    "            if word in vocab:\n",
    "                word_idx = vocab[word]\n",
    "                embeddings[word_idx] = np.asarray(vec)\n",
    "        except:\n",
    "            pass\n",
    "        line = file_r.readline()\n",
    "    np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "\n",
    "def get_training_word2vec_vectors(filename):\n",
    "    with np.load(filename) as data:\n",
    "        return data[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_word_filename='vector_word.txt'  #vector_word trained by word2vec\n",
    "train_word2vec(file_list ,vector_word_filename)\n",
    "vector_word_npz='vector_word.npz'   # save vector_word to numpy file\n",
    "# trans vector file to numpy file\n",
    "if not os.path.exists(vector_word_npz):\n",
    "    export_word2vec_vectors(word_to_id,\n",
    "                            vector_word_filename,\n",
    "                            vector_word_npz)\n",
    "pre_trianing = get_training_word2vec_vectors(vector_word_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename,word_to_id,cat_to_id,max_length=600):\n",
    "    labels,contents=read_file(filename)\n",
    "    data_id,label_id=[],[]\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "    x_pad=keras.preprocessing.sequence.pad_sequences(data_id,\n",
    "                                                     max_length,\n",
    "                                                     padding='post', \n",
    "                                                     truncating='post')\n",
    "    y_pad=keras.utils.to_categorical(label_id)\n",
    "    return x_pad,y_pad\n",
    "\n",
    "train_file_name = \"sample__train_0.2.txt\"\n",
    "x_train,y_train = process_file(train_file_name ,\n",
    "                               word_to_id,\n",
    "                               cat_to_id,\n",
    "                               max_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file_name = \"sample__valid_0.2.txt\"\n",
    "x_valid,y_valid = process_file(valid_file_name ,\n",
    "                               word_to_id,\n",
    "                               cat_to_id,max_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"int\")\n",
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding  = get_training_word2vec_vectors(\"vector_word.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras \n",
    "import tensorflow as tf \n",
    "from  tensorflow.keras import Sequential ,Model\n",
    "from  tensorflow.keras.layers import Input ,Flatten ,Dropout , \\\n",
    "                            Embedding ,Conv1D,MaxPooling1D ,Dense\n",
    "\n",
    "max_length = 600 \n",
    "model = Sequential()\n",
    "main_input = Input(shape=( max_length), dtype='float64')\n",
    "embedder = Embedding(vocab_size, \n",
    "                     100, \n",
    "                     input_length=x_train.shape[0],\n",
    "                     weights=[train_embedding],\n",
    "                     trainable=False)\n",
    "#embedder = Embedding(len(vocab) + 1, 300, input_length=50, trainable=False)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = MaxPooling1D(pool_size=38)(cnn1)\n",
    "cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = MaxPooling1D(pool_size=37)(cnn2)\n",
    "cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = MaxPooling1D(pool_size=36)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = tf.keras.layers.concatenate([cnn1, cnn2, cnn3], axis=1)\n",
    "flat = Flatten()(cnn)\n",
    "drop = Dropout(0.2)(flat)\n",
    "main_output = Dense(10, activation='softmax')(drop)\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size= 64 , \n",
    "          epochs=20 ,\n",
    "         validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
