Thinking1	在实际工作中，FM和MF哪个应用的更多，为什么? 
MF(矩阵分解) ，FM（因子分解机） ， 在实际工作中 ，FM使用更广泛 。 
1、能简要说明FM和MF的区别？
	1） MF 只能解决user ,item 之间的两两特征之间的关联，但是实际工作中特征的维度往往超过两维度  。 
	2） MF 中user_item 矩阵进行One_Hot编码之后，带来高维度的稀疏性 。MF进行学习的时候， 不能学习到两两特征的交叉特征的隐含关系，存在用户冷启动的问题 。 
		但是FM通过隐向量的内积来提取特征组合，对于训练数据中很少或没有出现的特征组合也能够学习到 ，一定程度解决高度稀疏带来的问题 。 
	3）矩阵分解MF是FM的特例，即特征只有User ID 和Item ID的FM模型 。
	4）矩阵分解MF只适用于评分预测，进行简单的特征计算，无法利用其他特征。
	5）FM引入了更多辅助信息（Side information）作为特征 。
	6) FM在计算二阶特征组合系数的时候，使用了MF。
2、能简要说明FM在推荐系统，以及应用场景中的作用 。
	FM可以应用于很多预测任务，比如回归、分类、排序等等。
    1.回归Regression：y^(x)直接作为预测值，损失函数可以采用least square error；
    2.二值分类Binary Classification：y^(x)需转化为二值标签（比如sigmod变换 ，softmax变化等），如0,1。损失函数可以采用logit loss(交叉熵)
    3.排序Rank：x可能需要转化为pair-wise的形式如(X^a,X^b)，损失函数可以采用pairwise loss
	
Thinking2	FFM与FM有哪些区别
	FFM 与FM都是利用隐向量之间的内积进行提取特征组组合 。 
	区别如下： 
		FM中每一个Categorical类的特征进行One—Hot编码编码之后，便失去了类别的信息， 
		FFM中任然利用categorical的信息， 引入了同种性质的特征属于同一个Fiel的概念。 
		因此， FM可以视为所有的特征属于同一个Field 的特例 。 
		在FFM中， 每一维X_i , 针对其他filed不仅可以学到一个隐变量 ，因此，隐向量不仅与特征相关，
		而且与filed相关。结论就是不同Filed 之间的变量学习到不同的隐向量 ， 
		这和实际不同filed 之间本身就存在不同的差异相符合 。  
		由于FFM引入了filed概念， 因此计算没法进行简化  , 导致计算复杂度为(KN^2) ，相比FM可以优化时间复杂度到（KN) 
		其中K 为隐向量的长度 。 

Thinking3	DeepFM相比于FM解决了哪些问题，原理是怎样的？ 			       
	FM理论上可以进行三阶甚至更加高阶的特征组合（FM一般只是进行二阶特征的特征组合）， 但是将会带来大量的计算，
	因此可以将更加高阶的特征组合交给神经网络(DNN)进行学习 ，同时考虑低阶和高阶特征引入DeepFM,利用deep层更加提取到高阶特征的信息关联。
	FM层比较适合处理海量稀疏特征 ， 但是DNN模型对于海量稀疏特征学习的过程中可能导致梯度消失的现象 。 
	因此在FM, dnn层中间插入embedding 层, 既满足dnn网络模型输入的要求， 又对高维的特征进行降维 。
	DeepFM中的模块：
		• Sparse Features，输入多个稀疏特征
		• Dense Embeddings
			对每个稀疏特征做embedding，学习到他们的embedding向量(维度相等，均为k），因为需要将这些embedding向量送到FM层做内积。同时
			embedding进行了降维，更好发挥Deep Layer的高阶特征学习能力
		• FM Layer
			一阶特征：原始特征相加
			二阶特征：原始特征embedding后的embedding向量两两内积
		• Deep Layer，将每个embedding向量做级联，然后做多层的全连接，学习更深的特征
		• Output Units，将FM层输出与Deep层输出进行级联，接一个dense层，作为最终输出结果

Thinking4	假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。
	基于内容进行推荐的步骤及原理？ 
	步骤： 
		第一步 :  文章摘要进行提取特征。分词处理(包括去掉常用的停用词)等
		第二步 ： 生成词向量（物品的特征向量），可以使用词频统计， 也可以使用TF-idf 
		第三步 :  对用户看过的小说，基于相似度矩阵, 推荐相似度最高的几部小说  
	原理:
		用户在短时期内兴趣不会发生大幅度的改变， 因此基于相似矩阵进行推荐。 使用content_based 只是利用item（小说）的静态属性。

		
Thinking5	Word2Vec的应用场景有哪些
	答： NLP领域中Word2Vec可以用于计算文本相似度 ， 每一个词语通过一个向量进行表达，学习的embedding向量可以输入给其他模型 比如text_cnn 进行文本信息分类， 或者情感分析。 
		社交网络大V推荐：
			在社交网络中，给当前用户推荐 他/她 可能关注的大V  。 
			映射关系： 大V是单词， 用户关注大V的顺序就是文章 
		App推荐：
			App 商店中，向用户推荐感兴趣的 App
			映射关系：每个 App 就是一个词；将每个用户下载的 App，按照下载的顺序排列，形成文章
		广告系统
		应用场景：广告主在媒体网站上打广告，媒体网站提供一个后台管理系统，可以让广告主自行决定要将广告推荐给哪些目标人群。
		映射关系：每一个页面就是一个词；将每个用户浏览的页面，按照浏览的顺序排列，形成文章。
