Thinking1	什么是近似最近邻查找，常用的方法有哪些	？
在机器学习领域，语义检索，推荐系统等方向常涉及到的一个问题是：给定一个向量X=[x1,x2,x3...xn]，需要从海量的向量库中找到最相似的前K个向量。通常这些向量的维度很高，对于在线服务，用传统的方法查找是非常耗时的，容易使得时延上成为瓶颈，因此业界通用的方式就是将最相似的查找转换成Ann问题。这样查找返回的前K个向量并不一定是最相似的K个向量，衡量Ann算法好不好的一个依据是召回，每次Ann请求返回的K个结果与使用暴力查找的K个结果去比较,如果完全一致，说明是最好的。因为省了搜索时间却没有影响效果。
常用的方法有：LSH  ,K-d Tree( -> ball Tree ) 等等， 大致常用hash 函数技术 ，以及树模型划分参数控件的办法。 
LSH（Locality sensitive hashing）其核心思想是：在高维空间相邻的数据经过哈希函数的映射投影转化到低维空间后，他们落入同一个吊桶的概率很大而不相邻的数据映射到同一个吊桶的概率则很小。在检索时将欧式空间的距离计算转化到汉明（Hamming）空间，并将全局检索转化为对映射到同一个吊桶中的数据进行检索，从而提高了检索速度。

Thinking2	为什么两个集合的minhash值相同的概率等于这两个集合的Jaccard相似度		? 
具体地，min-hash就是将行（不同向量的每一个维度）多次随机打乱，找出第一个非零行的索引序号，作为最小哈希值h(i)

两个集合（比如a和b集合）进行0-1编码。对应位置上a_i和b_i共有三种情况。
• A类：两列的值都为1；
• B类：其中一列的值为0，另一列的值为1；
• C类：两列的值都为0

根据Jaccard相似度的定义可知，某两个集合的交集除以这两个集合的并集 即 a/(a+b)
根据minhash的定义（特征矩阵按行进行随机排列后，第一个列值为1的行的行号）以及概率论中排列组合的知识（无穷多次）C类行对于结果计算没有影响，可以删除P(h(Ci)=h(Cj))=P(删掉C类后，第一行为A类)  , A类行的个数/所有行的个数=a/(a+b) , 因此P(h(Ci)=h(Cj))= sim(Ci,Cj)
	
Thinking3	SimHash在计算文档相似度的作用是怎样的？			
1 . SimHash是将一段文本hash成一串二进制的指纹（如0010110），然后配用海明距离进行两两文本的比较。海明距离，说白了就是看两段二进制指纹有多少不一样。一般来说，如果海明距离小于3，则认为这两个文本是相似文本 。 
（1）海明距离定义
    海明距离为两串向量中，对应元素不一样的个数，比如101010与101011的最后一位不一样，那么hamming distance即为1,，同理000与111的hamming为3。
    但这没有考虑到向量的长度，如111111000与111111111的距离也是3，尤其是比较文本的相似时，这样的结果肯定不合理，因此我们可以用向量长度作为分母。Python 中的 hamming distance 即这么计算的。
    海明距离也是值越小越相似。但除以长度之后的海明距离，最大值为1（完全不相似），最小值为0（完全一致）。
2、如何通过文档的SimHash计算文档之间的相似度（5points）
    流程：
        1） 分词、给定初始权重
        2） 传统hash
        3） 统计并加权  
        4）  合并
        5） 0/1 处理
    简单的将，一篇文档的simhash（64位）是这样计算的：定义一个长度为64的数组s[]，初始化为0。针对文档中的每一个词，计算词的hash（64位），如果hash第i位为1，则将数组s[i]加1，否则s[i]减1。所有的词计算完毕后，将数组收缩：如果>0，置为1，否则置为0。得到64个0或1数字，组合成64位的数字，即为simhash。

    直接对海量文档比较文档指纹开销较大。采用另一种方法，如果汉明距离为N以内相似度较高，就可以将指纹分为N+1段。如果有一段相同，那么文档相似的可能性就很大，可以通过这个方法，降低计算成本。（抽屉原理 ，但是需要备份hashcode ） 

Thinking4	为什么YouTube采用期望观看时间作为评估指标	? 
    youtube 推荐系统中 ， 视频观看过程中， 往往遇到标题党 ，假大空的现象， 用户点击视频过后，发现视频与期望不符，很快就结束观看了。 
    因此，采用CTR预估指标进行推荐， 长时间观看和快速点击没有进行区分， 但是对CTR预估问题的贡献是一致的 ，先让不符合推荐场景。(因为我们有理由 相信观看时间长的用户更加喜欢自己观看的是视频 ) 。 因此，采用观看时间作为正例继续建模， 合情合理。 而负样本为无点击视频，输出值则统一采用1，即采用单位权值，不进行加权。 

Thinking5	为什么YouTube在排序阶段没有采用经典的LR（逻辑回归）当作输出层，而是采用了Weighted Logistic Regression？			

• CTR指标对于视频搜索具有一定的欺骗性，所以论文提出采用期望观看时间作为评估指标
• 观看时长不是只有0，1两种标签，所以YouTube采用了Weighted Logistic Regression来模拟这个输出
• 划分样本空间时，正样本为点击，输出值即阅读时长值；负样本为无点击视频，输出值则统一采用1，即采用单位权值，不进行加权
• 在正常的逻辑回归中，几率odds计算，表示样本为正例概率与负例概率的比例引入观看时间时，LR学到的odds为表示正例出现概率的和与负样本概率的和的比例
