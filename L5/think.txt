thinking1	什么是矩阵分解，都有哪些应用场景？		
答 ： 
	什么是矩阵分解 :
			矩阵分解一般用于预测rating 矩阵中缺失的值,一般rating 矩阵中存在大量的缺失值 （rating 矩阵十分稀疏 ） 。 
			通过MF 是的预测出来的空白部分一定程度上反映用户喜欢items之间的优先级关系 。通过已经知道的真是数据数据与预测数据进行loss 的计算 		，转化成最优化问题进行求解。 
	应用场景: 
			矩阵分解的应用场景：
				1. 完全个性化推荐中 ，一般用于评分预测的问题， 也就是矩阵补全的问题。
				2 . 利用矩阵分解得到的item K为特征向量可一计算物品之间的相似度， 进行item_based TopK推荐 。 
				3. 利用矩阵分解得到的user k 维特征向量计算用户之间的相似度， 进行user_based TopK 推荐 。 
				4 . 利用矩阵分解得到的  user k维特征向量进行用户聚类进行用户打标签  。 
					
			
thinking2	"矩阵分解算法ALS-WR是如何进行的？			
	ALS适用于用户对商品的有明确的评分矩阵的场景，然而很多情况下用户没有明确的反馈对商品的偏好，而是通过一些行为隐式的反馈。
	比如对商品的购买次数、对电视节目收看的次数或者时长，这时我们可以推测次数越多，看得时间越长，用户的偏好程度越高，
	但是对于没有购买或者收看的节目，可能是由于用户不知道有该商品，或者没有途径获取该商品，我们不能确定的推测用户不喜欢该商品。
	ALS-WR通过置信度的权重来解决此问题，对于我们更确信用户偏好的项赋予较大的权重，对于没有反馈的项，赋予较小的权重。
	目标函数： 为了找到低维矩阵X,Y最大程度地逼近矩分矩阵R，最小化平方误差损失函数。	加入正则项降低过拟合风险 。 
	ALS-WR模型 
	ALS-WR 计算过程如下：
		对隐式矩阵进行分解： 引入置信度的概念 。 	
		step1 . 固定X  , 更新Y 
		step2  .固定y   ,更新X 
		step3 .  不同重复step1, step2 迭代计算直到计算结果达到满意的结果。 
				 

thinking3 梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）			
	批量梯度下降算法选取所有的训练样本进行参数更新,计算速度慢 ， 一般可以保证整体一定就是向梯度下降的方向更新参数的 。
	由于批梯度下降每跟新一个参数的时候，要用到所有的样本数，所以训练速度会随着样本数量的增加而变得非常缓慢。
	随机梯度下降正是为了解决这个办法而提出的。它是利用每个样本的损失函数对θ求偏导得到对应的梯度，来更新θ：
	随机梯度下降是通过每个样本来迭代更新一次，对比上面的批量梯度下降，迭代一次需要用到所有训练样本（往往如今真实问题训练数据都是非常巨大），
	一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 
	Minibatch更新参数则介于两者之间 ， 通过随机化的batch数据量的数据进行参数更新， 一定程度避免跳出最优 的值 ，又大大加快了训练速度 。
	深度学习中一般采用Minibatch 的方式 。
		

thinking4	推荐系统中的冷启动都有哪些情况，有哪些常用的解决方法			
		推荐系统中冷启动的问题 有新注册的用户， 新商品上架  ，系统刚刚部署是上线运等等。 
		用户冷启动的常用方法有:
				1. 提供非个性化的推荐（比如热门topN） 
				2 .使用基于内容的推荐(content_based)算法
				3. 利用用户注册的信息进行快速试探用户的兴趣爱好。
				4. 利用其他系统该用户的信息导入继续推荐。 
		物品冷启动的常用方法有 ：
				1 . 利用物品本身的标签进行推荐 。
				2 . 基于关系传递关系进行推荐 。 
		系统冷启动的常用方法有： 
				1 . 使用基于内容的推荐(content_based)算法 。
				2 . 利用其他系统该用户的信息导入继续推荐 。
				3. 利用用户注册的信息进行快速试探用户的兴趣爱好

thinking5	你阅读过和推荐系统，机器学习相关的论文么？有哪些论文是你比较推荐的，可以分享到微信群中			
				1. 	 ALS并行加速 的论文  ， 大大提高计算效率  。  
				2.  SlopeIOne 推荐算法  工程实现容易 。 
				

