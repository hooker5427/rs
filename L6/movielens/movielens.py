# -*- coding: utf-8 -*-
"""使用surprise包.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Md_UUj4_HAgAWtU64Hgr9O6zWhaKNnoD
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/My Drive/Colab Notebooks')

!ls

!pip install surprise

"""使用surprise SVD系列算法推荐"""

from surprise import Dataset
from surprise import Reader
from surprise.prediction_algorithms import SVD ,SVDpp  
from surprise import accuracy
from surprise.model_selection import KFold
import pandas as pd
import  os

reader = Reader(line_format='user item rating', sep=',', skip_lines=1)
data = Dataset.load_from_file( "./ratings.csv" , reader = reader) 
from surprise.model_selection import train_test_split 

x_train , x_test = train_test_split( data ,test_size = 0.2 ,random_state = 10000 )
svd = SVD(biased= False) 
svd.fit(x_train)

prediction = svd.test(x_test)
accuracy.rmse( predictions=prediction )
# RMSE: 0.8548
# 0.8547798833361556

import pandas as pd 
import numpy as np
datas = pd.read_csv("ratings.csv" ,delimiter="," , skiprows=1  , names  =["user" ,"item" ,"rating"] ,usecols= [0,1,2] )
datas["user"] = datas["user"].astype(np.int32)
datas["item"] = datas["item"].astype(np.int32)
datas["rating"] = datas["rating"].astype(np.int32)
print(datas.dtypes )

reader = Reader(line_format='user item rating' )
data = Dataset.load_from_df( datas ,reader ) 
from surprise.model_selection import train_test_split 

x_train , x_test = train_test_split( data ,test_size = 0.2 ,random_state = 10000 )
svd = SVD(biased= False) 
svd.fit(x_train)

prediction = svd.test(x_test)
accuracy.rmse( predictions=prediction )

from  collections import defaultdict
def get_top_n(predictions, n=10):
    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]
    return top_n

top_n = get_top_n(prediction, n=10)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
    print(uid, [iid for (iid, _) in user_ratings])

def precision_recall_at_k(predictions, k=10, threshold=3.5):
    '''Return precision and recall at k metrics for each user.'''

    # First map the predictions to each user.
    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():
        # Sort user ratings by estimated value
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        # Number of relevant items
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)
        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])
        # Number of relevant and recommended items in top k
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[:k])
        # Precision@K: Proportion of recommended items that are relevant
        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1
        # Recall@K: Proportion of relevant items that are recommended
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1
    return precisions, recalls


precisions, recalls = precision_recall_at_k( prediction, k=5, threshold=4)

# Precision and recall can then be averaged over all users
print(sum(prec for prec in precisions.values()) / len(precisions))
print(sum(rec for rec in recalls.values()) / len(recalls))

# SVDpp 
svd = SVDpp() 
svd.fit(x_train)

prediction = svd.test(x_test)
accuracy.rmse( predictions=prediction )
top_n = get_top_n(prediction, n=10)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
  print(uid, [iid for (iid, _) in user_ratings])

# SVD
svd = SVD() 
svd.fit(x_train)

prediction = svd.test(x_test)
accuracy.rmse( predictions=prediction )
top_n = get_top_n(prediction, n=10)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
  print(uid, [iid for (iid, _) in user_ratings])

# 交叉验证
from surprise.model_selection.validation import cross_validate

algo =SVD(biased= False) 
res = cross_validate(algo, data, measures=['rmse', 'mae'], 
              cv= 3, return_train_measures=False ,
              verbose=False)
print(res )

# 网格搜索
# GrideSearchCV
from surprise.model_selection.search import GridSearchCV

param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],
              'reg_all': [0.4, 0.6] , 'biased' :[False]          }


gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(data)

# best RMSE score
print(gs.best_score['rmse'])
# combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

# We can now use the algorithm that yields the best rmse:
model = gs.best_estimator['rmse']
model.fit(x_train)

predictions = model.predict(x_test)
top_n = get_top_n(predictions , n=10)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
  print(uid, [iid for (iid, _) in user_ratings])

"""使用xlearn"""

!pip install xlearn

# xlearn
import numpy as np
import xlearn as xl
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error 
import pandas as pd 


columns ="user_id,movie_id,rating,timestamp,title,genres,gender,age,occupation,zip".split(",")
df = pd.read_csv("movielens_sample.txt"  ,names = columns  ,skiprows=1)

# 去掉 time , title 列
df = df.drop( ["timestamp" ,"title" ],axis  =1 )

df  = df.genres.str.get_dummies(sep ="|").join(df).drop("genres" ,axis =1 )
df  = df.gender.str.get_dummies(sep ="|").join(df).drop("gender" ,axis =1 )
from sklearn.preprocessing import LabelEncoder 
enc = LabelEncoder()
df['zip'] = enc.fit_transform(df['zip'])

#  数据集切分
y  ,X = df.pop('rating') ,df
X_train , X_test ,y_train ,y_test = train_test_split( X ,y ,test_size =0.1)
# DMatrix transition
xdm_train = xl.DMatrix(X_train, y_train)
xdm_test = xl.DMatrix(X_test, y_test)



# Training task
fm_model = xl.create_fm()  # Use factorization machine
# we use the same API for train from file
# that is, you can also pass xl.DMatrix for this API now
fm_model.setTrain(xdm_train)    # Training data
fm_model.setValidate(xdm_test)  # Validation data

# 默认early_stoping ....  
param = {'task':'reg', 'lr':0.2, 
         'lambda':0.002, 
         'metric':'rmse' ,
         'epoch'  :10 , 
        'opt' :'sgd'}

fm_model.fit(param, './model_dm.out')

fm_model.setTest(xdm_test)  # Test data

# Start to predict
# The output result will be stored in output.txt
# if no result out path setted, we return res as numpy.ndarray

y_pred = fm_model.predict("./model_dm.out")
print(y_pred)



# 计算rmse 
print('MSE为：',mean_squared_error(y_test,y_pred))
# print('MSE为(直接计算)：',np.mean((y_test-y_pred)**2))
print('RMSE为：',np.sqrt(mean_squared_error(y_test,y_pred)))


# error  module 'xlearn' has no attribute 'DMatrix' 
#  好像不可以 pip install 安装
# 源码安装成功运行





