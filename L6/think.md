#### 奇异值分解SVD的原理是怎样的，都有哪些应用场景？ 
+ 1. 普遍适用性
    - 类似于特征分解将矩阵分解成特征向量和特征值，奇异值分解(SVD)将矩阵分解为奇异向量（singular vector）和奇异值（singular value。通过分解矩阵，我们可以发现矩阵表示成数组元素时不明显的函数性质。而相比较特征分解，奇异值分解有着更为广泛的应用，这是因为每个实数矩阵都有一个奇异值分解，但未必都有特征分解。例如，非方阵型矩阵没有特征分解，这时只能使用奇异值分解。
+ 2.定义
    - 对于一个任意实数的矩阵A都可以分解成为三个矩阵相乘的形式. 
        -   $A = U \Sigma V^T = \sum^{\min{m,n}}_{i=1} \delta_i u_i v_i^T$
        其中，U是一个$m \times m$的正交矩阵，$\Sigma$是一个$m \times n$的对角矩阵，V是一个$n \times n$的正交矩阵。注意，矩阵$\Sigma$不一定是方阵。矩阵$\Sigma$中的非零元素（即对角线上的元素）被称为矩阵A的奇异值，矩阵U和矩阵V的列向量分别被称为左奇异向量以及右奇异向量。事实上，A的相关特征分解可以帮助我们解释A的奇异值分解，A的左奇异向量是$AA^T$的特征向量，A的右奇异向量是$A^TA$的特征向量。A 的非零奇异值是 $A^TA$ 特征值的平方根，同时也是 $AA^T$特征值的平方根。
+ 3.应用场景
    - 3.1低秩矩阵（原理）
        - 举例：低秩矩阵(该理论在计算机视觉中应用广泛)对于某个矩阵A，我们试图找到他的一个低秩矩阵B，使得B的矩阵特性近似于A，即：$$B = \sum^{k}_{i=1}x_i y_i^T$$
        - 低秩矩阵估算定理
            $$B_k = U_k \Sigma_k V_k^T = \sum^{k}_{i=1} \delta_i u_i v_i^T$$
        其中，
        $U_k$为$m \times k$矩阵，由矩阵U的前k个列向量组成；
        $\Sigma_k$为一个对角矩阵，由前k个奇异值组成；
        $V_k$为$n \times k$矩阵，由矩阵V的前k个列向量组成。
        $B_k$实质上是一个被缩减的奇异值分解（truncated SVD）。
     - 3.2  压缩矩阵进行推荐系统 
        - 用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐 。 
    - 3.3 PCA降维 
        - 分析了解原矩阵的主要特征和携带的信息（取若干最大的奇异值）， 可以利用SVD实现主成分分析PCA 。 
    - 3.4 图片有损压缩
        - 丢弃忽略原矩阵的次要特征和携带的次要信息（丢弃若干较小的奇异值），这引出了图片进行信息有损压缩 。

#### Thinking2	 funkSVD, BiasSVD , SVD++算法之间的区别是怎样的?
+ 1、能简述3种算法之间的差异?
    FunkSVD是在传统SVD面临计算效率问题时提出来的，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在期望我们的矩阵$M$这样进行分解：
    #### $M= P^TQ$
    采用了线性回归的思想。我们的目标是让用户的评分和用矩阵乘积得到的评分残差尽可能的小，也就是说，可以用均方差作为损失函数，来寻找最终的P和Q。对于某一个用户评分$m_{ij}$，如果用FunkSVD进行矩阵分解，则对应的表示为qTjpi，采用均方差做为损失函数，则我们期望$(m_{ij} - q^T_jP_i)^2$尽可能的小，如果考虑所有的物品和样本的组合，则我们期望最小化下式:
    #####   $ argmin \sum_{i,j}(m_{ij} - q^T_jP_i)^2$
　　只要我们能够最小化上面的式子，并求出极值所对应的$p_i$ ,$q_i$ , 我们最终可以得到矩阵$P$和$Q$，那么对于任意矩阵M任意一个空白评分的位置，我们可以通过$q^Tp_i$计算预测评分。

+ 在FunkSVD算法之后，出现了很多FunkSVD的改进版算法。其中BiasSVD算是改进的比较成功的一种算法。BiasSVD假设评分系统包括三部分的偏置因素：一些和用户物品无关的评分因素，用户有一些和物品无关的评分因素，称为用户偏置项。而物品也有一些和用户无关的评分因素，称为物品偏置项。这其实很好理解。  用户有自己的偏好(Bias)，比如乐观的用户打分偏高商品也有自己的偏好(Bias) ,比如质量好的商品，打分偏高.将与个性化无关的部分，设置为偏好(Bias)部分 。 
+ SVD++算法在BiasSVD算法上进一步做了增强，这里它增加考虑用户的隐式反馈。隐式反馈：没有具体的评分，但可能有点击，浏览等行为 。 

#### Thinking3	矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足？ 
+ 1、说明推荐系统中的典型应用场景？
    + 说明： 
        - 矩阵分解算法的核心思想是将用户行为矩阵分解为两个低秩矩阵的乘积，通过分解，我们分别将用户和标的物嵌入到了同一个k维的向量空间(k一般很小，几十到上百)，用户向量和标的物向量的内积代表了用户对标的物的偏好度。所以，矩阵分解算法本质上也是一种嵌入方法。
        上面提到的k维向量空间的每一个维度是隐因子(latent factor)，之所以叫隐因子，是因为每个维度不具备与现实场景对应的具体的可解释的含义，所以矩阵分解算法也是一类隐因子算法。这k个维度代表的是某种行为特性，但是这个行为特性又是无法用具体的特征解释的，从这点也可以看出，矩阵分解算法的可解释性不强，我们比较难以解释矩阵分解算法为什么这么推荐。
        矩阵分解的目的是通过机器学习的手段将用户行为矩阵中缺失的数据(用户没有评分的元素)填补完整，最终达到可以为用户做推荐的目标。
    - 应用场景
        1) 个性化推荐
        2) 通过矩阵分解得到的K维向量可以进行相似度计算 ,实现topk 物品推荐。 
        3) 通过矩阵分解我们获得了用户和物品的k维特征向量，有了特征向量，我们就可以对用户和标的物聚类了。
        4) 作为其他模型的特征输入， deepFM 等 。 
+ 2、MF在推荐系统中的局限性?	
    1. 可解释性不强 。
    2. 不能应用与在线实时推荐 。 
    3. 存在用户冷启动的问题  。
    4. 实现上仅仅只是考虑单一特征， 没有进行特征交叉  。

#### Thinking4：item流行度在推荐系统中有怎样的应用
+ 1、冷启动中的使用 
    -  新用户信息较小的时候，采用非个性推荐算法进行推荐(基于流行度的方式)
    - 对于用户冷启动的问题，  使用居于流行度的推荐算法 ，有利于发掘用于的爱好兴趣， 提高用户兴趣的多样性。 
+ 2、协同过滤中的TopN推荐 
    - 进行协同过滤个性化推荐的时候，需要考虑商品的流行度。 通过对热门商品进行一定程度的降权处理 。 显然， 给用户推荐5个商品， 相似用户A中物品中4件都是热门流行商品， 但是相似用户B的物品推荐列表中只有两件是热门商品， 可以相似用户B的相似列表更加可信， 更加利于个性化推荐。
    - 进行TopN推荐的时候，既考虑反馈次数，也要考虑流行度的降权影响 。 
    - 对于忠实用户，可以考虑高流行度对商品推荐的降权影响，挖掘长尾 。 
+ 3、其他使用 
    + 改进userCF
        - 基于userCF的协同过滤算法在进行相似度的计算的时候采用余弦相似度或者杰森相似度进行推荐的时候 ，只是考虑用户之间的相似程度， 但是用户的相似程度收到人们商品的影响， 因为可能热门商品大家都十分的喜欢，为了更加个性化的推荐，在一定程度上减少热门商品的推荐， 借鉴TD-IDF的思想， 对热门商品进行软惩罚， 引入USER-IIF  对热门商品进行一定程度的惩罚。 
        -  举例说明：
            - 如果两个用户都曾经买过《新华字典》，这丝毫不能说明他们兴趣相似， 因为绝大多数中国人小时候都买过《新华字典》。但如果两个用户都买过《数据挖掘导论》，那可 以认为他们的兴趣比较相似，因为只有研究数据挖掘的人才会买这本书。换句话说，两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度。
    + 改进itemCF 
        - 在协同过滤中两个物品产生相似度是因为它们共同出现在很多用户的兴趣列表中。换句话说，每个用户的兴趣列表都对物品的相似度产生贡献。但是每个用户的贡献不相同 。
        - 举例说明：
            - 假设有这么一个用户，他是开书店的，并且买了当当网上80%的书准备用来自己卖。那么， 他的购物车里包含当当网80%的书。假设当当网有100万本书，也就是说他买了80万本。这个用户虽然活跃，但是买这些书并非都是出于自身的兴趣，而且这些书覆盖了当当网图书的很多领域，所以这个用户对于他所购买书的两两相似度的贡献应该远远小于一 个只买了十几本自己喜欢的书的文学青年。为了解决这个问题，John S. Breese一个称为IUF(Inverse User Frequence)，即用户活跃度对数的倒数的参数，他认为活跃用户对物品相似度的贡献应该小于不活跃的用户，他提出应该增加IUF 参数来修正物品相似度的计算公式（ItemCF-IUF，对活跃用户进行软性惩罚。 

#### Thinking5	推荐系统的召回阶段都有哪些策略			
+ 1、能说出3种以上的召回策略（10points）
    - 1）  Content_based 
        - a) 基于标签
        - b) 基于兴趣
        - c) 基于热门 ，流行度
    - 2）  协同过滤
        -a) 基于矩阵（MF）
        -b) 隐语义模型(svd ，FM  ， FFM ) 
    - 3）  知识图谱
        - ) 用户之间静态属性之间的联系  。 比如 ，电影推荐过程中 同一个演员， 同一个导演等等。
    - 4)   图神经网络
        -) 利用用户动态隐形的行为数据 ，建立用户-物品之间的二部图 。进行图中信息传递进行 。 



    
    